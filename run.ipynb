{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_data import data_transform, FacialKeypointsDataset  # 导入自定义的人脸关键点数据类\n",
    "from model import get_net  # 导入网络模型，也可以在该文件中定义\n",
    "\n",
    "\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**可视化测试结果**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def net_sample_output(test_loader, net):\n",
    "\n",
    "    # iterate through the test dataset\n",
    "    for i, sample in enumerate(test_loader):\n",
    "\n",
    "        # get sample data：images and ground truth keypoints\n",
    "        images = sample['image']\n",
    "        key_pts = sample['keypoints']\n",
    "\n",
    "        # convert images to FloatTensors\n",
    "        images, key_pts = images.to(devices[0]), key_pts.to(devices[0])\n",
    "\n",
    "        key_pts = key_pts.view(key_pts.size(0), -1)\n",
    "\n",
    "        # forward pass to get net output\n",
    "        output_pts = net(images)\n",
    "        \n",
    "        l = loss(key_pts, output_pts)\n",
    "        print(l / key_pts.size(0))\n",
    "\n",
    "        # reshape to batch_size x 68 x 2\n",
    "        output_pts = output_pts.view(output_pts.size()[0], 68, -1)\n",
    "\n",
    "        # break after first batch image is tested\n",
    "        if i == 0:\n",
    "            return images, output_pts, key_pts\n",
    "\n",
    "\n",
    "def show_all_keypoints(image, predicted_key_pts, gt_pts=None):\n",
    "    \"\"\"Show image with predicted keypoints\"\"\"\n",
    "    # image is grayscale\n",
    "    plt.imshow(image, cmap='gray')\n",
    "    plt.scatter(predicted_key_pts[:, 0], predicted_key_pts[:, 1], s=20, marker='.', c='m')\n",
    "    # plot ground truth points as green pts\n",
    "    if gt_pts is not None:\n",
    "        plt.scatter(gt_pts[:, 0], gt_pts[:, 1], s=20, marker='.', c='g')\n",
    "\n",
    "\n",
    "# visualize the output\n",
    "# by default this shows a batch of 10 images\n",
    "def visualize_output(test_images, test_outputs, gt_pts=None, batch_size=10):\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    for i in range(batch_size):\n",
    "        ax = plt.subplot(1, batch_size, i + 1)\n",
    "\n",
    "        # un-transform the image data\n",
    "        image = test_images[i].data  # get the image from it's wrapper\n",
    "        image = image.cpu().numpy()  # convert to numpy array from a Tensor\n",
    "        image = np.transpose(image, (1, 2, 0))  # transpose to go from torch to numpy image\n",
    "\n",
    "        # un-transform the predicted key_pts data\n",
    "        predicted_key_pts = test_outputs[i].data\n",
    "        predicted_key_pts = predicted_key_pts.cpu().numpy()\n",
    "        # undo normalization of keypoints\n",
    "        predicted_key_pts = predicted_key_pts * 50.0 + 100\n",
    "\n",
    "        # plot ground truth points for comparison, if they exist\n",
    "        ground_truth_pts = None\n",
    "        if gt_pts is not None:\n",
    "            ground_truth_pts = gt_pts[i]\n",
    "            ground_truth_pts = ground_truth_pts * 50.0 + 100\n",
    "\n",
    "        # call show_all_keypoints\n",
    "        show_all_keypoints(np.squeeze(image), predicted_key_pts, ground_truth_pts)\n",
    "\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**训练函数**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_net(num_epochs, net, train_iter,  lr, wd, devices, lr_period, lr_decay):\n",
    "\n",
    "    # prepare the net for training\n",
    "    net.train()\n",
    "\n",
    "    # constructer a optimizer\n",
    "    optimizer = torch.optim.Adam([{'params':net.features.parameters()},\n",
    "                                 {'params':net.output_new.parameters(), 'lr':6e-3}],\n",
    "                                lr = lr, weight_decay=wd)\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, lr_period, lr_decay)\n",
    "    num_batches, timer = len(train_iter), d2l.Timer()\n",
    "\n",
    "    legend = ['train loss']\n",
    "\n",
    "    animator = d2l.Animator(xlabel='epoch', xlim=[1, num_epochs],\n",
    "                            legend=legend)\n",
    "\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        metric = d2l.Accumulator(2)\n",
    "\n",
    "        # train on batches of data, assumes you already have train_loader\n",
    "        for batch_i, data in enumerate(train_loader):\n",
    "            images = data['image']\n",
    "            key_pts = data['keypoints']\n",
    "\n",
    "            timer.start()\n",
    "            images, key_pts = images.to(devices[0]), key_pts.to(devices[0])\n",
    "\n",
    "            # flatten pts\n",
    "            key_pts = key_pts.view(key_pts.size(0), -1).to(torch.float32)\n",
    "\n",
    "            # forward pass to get outputs\n",
    "            output_pts = net(images)\n",
    "\n",
    "            # calculate the loss between predicted and target keypoints\n",
    "            l = loss(output_pts, key_pts)\n",
    "            l = l.to(torch.float32)\n",
    "\n",
    "            # backward pass to calculate the weight gradients\n",
    "            l.backward()\n",
    "\n",
    "            # update the weights\n",
    "            optimizer.step()\n",
    "\n",
    "            # zero the parameter (weight) gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            metric.add(l, key_pts.shape[0])\n",
    "\n",
    "            timer.stop()\n",
    "\n",
    "            # print loss statistics\n",
    "            # to convert loss into a scalar and add it to the running_loss, use .item()\n",
    "            if (batch_i+1) % (num_batches // 5) == 0 or batch_i == num_batches -1:    # print every 10 batches\n",
    "                print('Epoch: {}, Batch: {}, Avg. Loss: {}'.format(epoch + 1, batch_i+1, metric[0]/metric[1]))\n",
    "                animator.add(epoch + (batch_i+1)/num_batches, (metric[0]/metric[1], None))\n",
    "\n",
    "        measures = f'train loss {metric[0] / metric[1]:.3f}'\n",
    "        scheduler.step()\n",
    "\n",
    "    print(measures + f'\\n{metric[1] * num_epochs / timer.sum():.1f}'\n",
    "                     f' examples/sec on {str(devices)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**定义网络模型**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_all_gpus():  #@save\n",
    "    \"\"\"返回所有可用的GPU，如果没有GPU，则返回[cpu(),]\"\"\"\n",
    "    devices = [torch.device(f'cuda:{i}')\n",
    "             for i in range(torch.cuda.device_count())]\n",
    "    return devices if devices else [torch.device('cpu')]\n",
    "\n",
    "# 可以从models.py中导入也可以自己在这里写\n",
    "def get_net(devices):\n",
    "    finetune_net = nn.Sequential()\n",
    "    finetune_net.features = torchvision.models.resnet101(pretrained=True)\n",
    "\n",
    "    # 改变输出层\n",
    "    finetune_net.output_new = nn.Sequential(nn.Linear(1000, 256),\n",
    "                                            nn.ReLU(),\n",
    "                                            nn.Linear(256, 136))\n",
    "    finetune_net.features.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "\n",
    "    # 模型参数转到cpu或可用的gpu上\n",
    "    finetune_net = finetune_net.to(devices[0])\n",
    "\n",
    "    # 冻结参数\n",
    "    # for param in finetune_net.features.parameters():\n",
    "    #     param.requires_grad = False\n",
    "\n",
    "    return finetune_net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**加载数据**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the transformed dataset\n",
    "transformed_dataset = FacialKeypointsDataset(csv_file='data/training_frames_keypoints.csv',\n",
    "                                                root_dir='data/training/',\n",
    "                                                transform=data_transform)\n",
    "\n",
    "# load training data in batches\n",
    "batch_size = 16\n",
    "\n",
    "train_loader = DataLoader(transformed_dataset,\n",
    "                            batch_size=batch_size,\n",
    "                            shuffle=True,\n",
    "                            num_workers=0)\n",
    "\n",
    "# create the test dataset\n",
    "test_dataset = FacialKeypointsDataset(csv_file='data/test_frames_keypoints.csv',\n",
    "                                        root_dir='data/test/',\n",
    "                                        transform=data_transform)\n",
    "\n",
    "test_loader = DataLoader(test_dataset,\n",
    "                            batch_size=batch_size,\n",
    "                            shuffle=True,\n",
    "                            num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**定义超参数以及损失函数**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "devices, num_epochs, lr, wd = try_all_gpus(), 10, 9e-5, 0\n",
    "\n",
    "lr_period, lr_decay, net = 2, 0.95, get_net(devices)\n",
    "\n",
    "# define the loss and optimization\n",
    "loss = nn.SmoothL1Loss(reduction='sum')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**开始训练**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_net(num_epochs, net, train_loader, lr, wd, devices, lr_period,\n",
    "    lr_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**保存网络参数**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model's paraments\n",
    "model_dir = 'saved_models/'\n",
    "model_name = 'keypoints_model_2.pt'\n",
    "\n",
    "# after training, save your model parameters in the dir 'saved_models'\n",
    "torch.save(net.state_dict(), model_dir + model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**加载网络参数**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load net paramenters\n",
    "state_dict = torch.load('./saved_models/keypoints_model_2.pt')\n",
    "net.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**在测试集上进行测试与可视化**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images, test_outputs, gt_pts = net_sample_output(test_loader, net)\n",
    "gt_pts = gt_pts.cpu().numpy()\n",
    "gt_pts = gt_pts.reshape(gt_pts.shape[0], -1, 2)\n",
    "# call it\n",
    "visualize_output(test_images, test_outputs, gt_pts)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1467613af31a6e7aac27461577f60c848de0d9f4dd59e991a1d2965458dcd209"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('mmd2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
